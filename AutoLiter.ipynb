{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "907435e0-306e-494f-a580-092b846a0912",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63bbc585-cd38-4187-9773-4478257c016c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests \n",
    "from urllib.parse import urlunsplit, urlsplit\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger('PDFs')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:27.0) Gecko/20100101 Firefox/27.0'}\n",
    "\n",
    "\n",
    "class pdfDownload(object):\n",
    "    def __init__(self):\n",
    "        self.sess = requests.Session()\n",
    "        self.sess.headers = HEADERS\n",
    "        \n",
    "    def set_proxy(self, proxy=None):\n",
    "        \"\"\"set proxy for session\n",
    "        \n",
    "        Args:\n",
    "            proxy (str): The proxy adress. e.g 127.0.1:1123\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if proxy:\n",
    "            self.sess.proxies = {\n",
    "                \"http\": proxy,\n",
    "                \"https\": proxy, }\n",
    "    \n",
    "    \n",
    "    def _get_available_scihub_urls(self):\n",
    "        '''\n",
    "        Finds available scihub urls via https://lovescihub.wordpress.com/ or \n",
    "        https://sci-hub.now.sh/\n",
    "        '''\n",
    "        urls = []\n",
    "        res = self.sess.get('https://lovescihub.wordpress.com/')\n",
    "        s = BeautifulSoup(res.content, 'html.parser')\n",
    "        for a in s.find('div', class_=\"entry-content\").find_all('a', href=True):\n",
    "            if 'sci-hub.' in a['href']:\n",
    "                urls.append(a['href'])\n",
    "        return urls\n",
    "    \n",
    "        \n",
    "    def fetch(self, url, auth=None):\n",
    "        '''Fetch pdf\n",
    "        \n",
    "        Args:\n",
    "            url (str):\n",
    "\n",
    "        Returns:\n",
    "            A dict OR None\n",
    "        '''\n",
    "        try:\n",
    "            r = self.sess.get(url, auth=auth)\n",
    "        \n",
    "            if r.headers[\"Content-Type\"] != \"application/pdf\":\n",
    "                logger.info(\"Failed to fetch pdf with url: {}\".format(url))\n",
    "            else:\n",
    "                return {\n",
    "                    'pdf': r.content,\n",
    "                    'url': url\n",
    "                    }\n",
    "        except:\n",
    "            logger.error(\"Failed to open url: {}\".format(url))\n",
    "    \n",
    "    \n",
    "    def get_pdf_from_direct_url(self, url, auth=None):\n",
    "        return self.fetch(url, auth=auth) \n",
    "    \n",
    "    \n",
    "    def get_pdf_from_sci_hub(self, identifier, auth=None):\n",
    "        '''Fetch pdf from sci-hub based on doi or url\n",
    "        \n",
    "        Args: \n",
    "            identifier (str): DOI or url\n",
    "            auth (tuple): (\"user\", \"passwd\")\n",
    "        \n",
    "        Returns:\n",
    "            A dict OR None\n",
    "        '''\n",
    "        for base_url in self._get_available_scihub_urls():\n",
    "            r = self.sess.get(base_url + '/' + identifier, auth=auth)\n",
    "            soup = BeautifulSoup(r.content, 'html.parser')\n",
    "            \n",
    "            pdf_div_names = ['iframe', 'embed']\n",
    "            for pdf_div_name in pdf_div_names:\n",
    "                pdf_div = soup.find(pdf_div_name)\n",
    "                if pdf_div != None:\n",
    "                    break \n",
    "            try:\n",
    "                url_parts = urlsplit(pdf_div.get('src'))\n",
    "                if url_parts[1]:\n",
    "                    if url_parts[0]:\n",
    "                        pdf_url = urlunsplit((url_parts[0], url_parts[1], url_parts[2], '', ''))\n",
    "                    else:\n",
    "                        pdf_url = urlunsplit(('https', url_parts[1], url_parts[2], '', ''))\n",
    "                else:\n",
    "                    pdf_url = urlunsplit(('https', urlsplit(base_url)[1], url_parts[2], '', ''))\n",
    "                    \n",
    "                return self.fetch(pdf_url, auth)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "        logger.info(\"Failed to fetch pdf with all sci-hub urls\")\n",
    "\n",
    "    def _save(self, content, path):\n",
    "        with open(path, \"wb\") as f:\n",
    "            f.write(content)\n",
    "            \n",
    "class crossrefInfo(object):\n",
    "    def __init__(self):\n",
    "        self.sess = requests.Session()\n",
    "        self.sess.headers = HEADERS\n",
    "        self.base_url = \"http://api.crossref.org/\"\n",
    "\n",
    "    def set_proxy(self, proxy=None):\n",
    "        \"\"\"set proxy for session\n",
    "        \n",
    "        Args:\n",
    "            proxy (str): The proxy adress. e.g 127.0.1:1123\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if proxy:\n",
    "            self.sess.proxies = {\n",
    "                \"http\": proxy,\n",
    "                \"https\": proxy, }\n",
    "            \n",
    "    \n",
    "    def extract_json_info(self, bib):\n",
    "        \"\"\"Extract bib json information from requests.get().json()\n",
    "        \n",
    "        Args:\n",
    "            bib (json object): obtained by requests.get().json()\n",
    "        \n",
    "        Returns:\n",
    "            A dict containing the paper information.\n",
    "        \"\"\"\n",
    "        pub_date = [str(i) for i in bib['published'][\"date-parts\"][0]]\n",
    "        pub_date = '-'.join(pub_date)\n",
    "\n",
    "        if 'author' in bib.keys():\n",
    "            authors = ' and '.join([i[\"family\"]+\" \"+i['given'] for i in bib['author'] if \"family\" and \"given\" in i.keys()])\n",
    "        else:\n",
    "            authors = \"No author\"\n",
    "\n",
    "        if 'short-container-title' in bib.keys():\n",
    "            try:\n",
    "                journal = bib['short-container-title'][0]\n",
    "            except:\n",
    "                journal = \"No journal\"\n",
    "        else:\n",
    "            try:\n",
    "                journal = bib['container-title'][0]\n",
    "            except:\n",
    "                journal = \"No journal\"\n",
    "\n",
    "        bib_dict = {\n",
    "            \"title\": bib['title'][0],\n",
    "            \"author\": authors,\n",
    "            \"journal\": journal,\n",
    "            \"year\": pub_date,\n",
    "            \"url\": bib[\"URL\"],\n",
    "            \"pdf_link\": bib[\"link\"][0][\"URL\"],\n",
    "            \"cited_count\": bib[\"is-referenced-by-count\"]\n",
    "        } \n",
    "        \n",
    "        return bib_dict\n",
    "\n",
    "\n",
    "    def get_info_by_doi(self, doi):\n",
    "        \"\"\"Get the meta information by the given paper DOI number. \n",
    "        \n",
    "        Args:\n",
    "            doi (str): The paper DOI number\n",
    "            \n",
    "        Returns:\n",
    "            A dict containing the paper information. \n",
    "            {\n",
    "                \"title\": xxx,\n",
    "                \"author\": xxx,\n",
    "                \"journal\": xxx,\n",
    "                etc\n",
    "            } \n",
    "            OR\n",
    "            None\n",
    "        \"\"\"\n",
    "        url = \"{}works/{}\"\n",
    "        url = url.format(self.base_url, doi)\n",
    "        \n",
    "\n",
    "        r = self.sess.get(url)\n",
    "\n",
    "        bib = r.json()['message']\n",
    "        return self.extract_json_info(bib)\n",
    "\n",
    "            \n",
    "    \n",
    "    def get_info_by_title(self, title):\n",
    "        \"\"\"Get the meta information by the given paper title. \n",
    "        \n",
    "        Args:\n",
    "            doi (str): The paper title\n",
    "            \n",
    "        Returns:\n",
    "            A dict containing the paper information. \n",
    "            {\n",
    "                \"title\": xxx,\n",
    "                \"author\": xxx,\n",
    "                \"journal\": xxx,\n",
    "                etc\n",
    "            }\n",
    "            OR\n",
    "            None\n",
    "            OR\n",
    "            A list [{}, {}, {}]\n",
    "        \"\"\"\n",
    "        url = self.base_url + \"works\"\n",
    "        params = {\"query.bibliographic\": title, \"rows\": 20}\n",
    "        try:\n",
    "            r = self.sess.get(url, params=params)\n",
    "            items = r.json()[\"message\"][\"items\"]\n",
    "            \n",
    "            for i, item in enumerate(items):\n",
    "                \n",
    "                title_item = item['title'][0]\n",
    "                try:\n",
    "                    title_item = title_item.decode(\"utf-8\")\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "                item[\"title\"][0] = title_item\n",
    "\n",
    "                if title_item.lower() == title.lower():\n",
    "                    return self.extract_json_info(item)\n",
    "                \n",
    "                items[i] = item\n",
    "\n",
    "            return [self.extract_json_info(it) for it in items]\n",
    "        except:\n",
    "            logger.error(\"Title: {} is error.\".format(title)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01ec7e05-98ed-4b4c-9ba0-0e85975bbad5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10.1038/s41586-019-1559-7',\n",
       " '10.1126/sciadv.adf2827',\n",
       " '10.1016/j.scib.2021.03.009',\n",
       " '10.1038/ncomms9657',\n",
       " '10.1038/s41558-021-01276-3']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "file_path = 'SST.md'\n",
    "\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    markdown_content = file.read()\n",
    "\n",
    "# Define a function to extract PDF filenames and DOI numbers from the markdown text\n",
    "def extract_doi(markdown_text):\n",
    "    # Regular expression patterns for extracting PDF filenames and DOI numbers\n",
    "    \n",
    "    doi_pattern = r\"\\{\\{(10\\.\\d{4,5}/[^\\}]+)\\}\\}\"\n",
    "\n",
    "    # Extract all matches\n",
    "    doi_numbers = re.findall(doi_pattern, markdown_text)\n",
    "\n",
    "    return doi_numbers\n",
    "\n",
    "# Extract PDF names and DOI numbers from the markdown content\n",
    "doi_numbers = extract_doi(markdown_content)\n",
    "\n",
    "# Display the extracted information\n",
    "doi_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2efd326-6796-4029-92e2-528d8f91abd7",
   "metadata": {},
   "source": [
    "1. 生成题录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fd8caca-5b1c-4ef0-863f-7c26edd2d2d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_line(doi):\n",
    "    crossref_info = crossrefInfo()\n",
    "    crossref_info.set_proxy(proxy=\"127.0.0.1:7890\")\n",
    "    \n",
    "    doi_numbers = extract_doi(doi)[0]\n",
    "    \n",
    "    bib_doi = crossref_info.get_info_by_doi(doi_numbers)\n",
    "    pdf = bib_doi['url'].split('/')[-1] + '.pdf'\n",
    "    bib = bib_doi\n",
    "    pdf_path = 'pdfs/' + pdf\n",
    "    replaced_literature = \"- **{}**. {} et.al. **{}**, **{}**, ([pdf]({}))([link]({})).\".format(\n",
    "                                    bib['title'], bib[\"author\"].split(\" and \")[0], bib['journal'], \n",
    "                                    bib['year'], pdf_path, \n",
    "                                    bib['url'])\n",
    "    return replaced_literature\n",
    "\n",
    "def extract_doi(markdown_text):\n",
    "    # Regular expression patterns for extracting PDF filenames and DOI numbers\n",
    "    \n",
    "    doi_pattern = r\"\\{\\{(10\\.\\d{4,5}/[^\\}]+)\\}\\}\"\n",
    "\n",
    "    # Extract all matches\n",
    "    doi_numbers = re.findall(doi_pattern, markdown_text)\n",
    "\n",
    "    return doi_numbers    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "686bf632-2350-42f8-a514-b3fe69ae5aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process each line in the markdown file\n",
    "import re\n",
    "file_path = 'SST.md'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    updated_lines = [process_line(line) for line in file]# Process each line in the markdown file\n",
    "with open('SST1.md', 'w', encoding='utf-8') as file:\n",
    "    for line in updated_lines:\n",
    "        file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd53df89-12f7-4075-a2ee-27f8e1303675",
   "metadata": {},
   "source": [
    "2. 下载pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "047a2029-a6bf-43be-9d0e-bf27e0a9ed05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deep learning for multi-year ENSO forecasts',\n",
       " 'A self-attention–based neural network for three-dimensional multivariate modeling and its skillful ENSO predictions',\n",
       " 'Unified deep learning model for El Niño/Southern Oscillation forecasts by incorporating seasonality in climate data',\n",
       " 'Increasing water cycle extremes in California and in relation to ENSO cycle under global warming',\n",
       " 'Enhanced risk of concurrent regional droughts with increased ENSO variability and warming']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfDownload.set_proxy(\"127.0.0.1:7890\")\n",
    "doi_pattern = r\"org\\/([\\w.\\/-]+)\"\n",
    "title_pattern = r\"\\*\\*(.*?)\\*\\*\"\n",
    "\n",
    "# Extract DOI numbers\n",
    "doi_numbers = [re.search(doi_pattern, entry).group(1) for entry in updated_lines if re.search(doi_pattern, entry)]\n",
    "titles = [re.search(title_pattern, entry).group(1) for entry in updated_lines if re.search(title_pattern, entry)]\n",
    "\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67c9dffd-d9d1-4e6f-af80-775955e012e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10.1038/s41586-019-1559-7',\n",
       " '10.1126/sciadv.adf2827',\n",
       " '10.1016/j.scib.2021.03.009',\n",
       " '10.1038/ncomms9657',\n",
       " '10.1038/s41558-021-01276-3']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doi_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "336e64bc-7581-453e-8e08-2b4f88042d44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://sci.bban.top/pdf/10.1038/s41586-019-1559-7.pdf\n",
      "error download1: sciadv.adf2827.pdf\n",
      "https://sci.bban.top/pdf/10.1016/j.scib.2021.03.009.pdf\n",
      "https://sci.bban.top/pdf/10.1038/ncomms9657.pdf\n",
      "error download4: s41558-021-01276-3.pdf\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(updated_lines)):\n",
    "    \n",
    "    pdf = doi_numbers[i].split('/')[-1] + '.pdf'\n",
    "    pdf_download = pdfDownload()\n",
    "    pdf_download.set_proxy(\"127.0.0.1:7890\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        pdf_dict = pdf_download.get_pdf_from_sci_hub(doi_numbers[i])\n",
    "        print(pdf_dict['url'])\n",
    "        pdf_download._save(pdf_dict['pdf'] ,'pdfs/' + pdf)\n",
    "    except:\n",
    "        print('error download' + str(i) + ': ' + pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0cff3e-326d-4f05-94f9-f58f4ad97ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
